package demo.rdd;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.scheduler.*;
import scala.Tuple2;
import scala.collection.JavaConverters;

import java.util.Arrays;
import java.util.List;
import java.util.concurrent.CountDownLatch;

public class VisualStageDivision {

  private static final CountDownLatch latch = new CountDownLatch(3);

  public static class DetailedListener extends SparkListener {
    @Override
    public void onJobStart(SparkListenerJobStart jobStart) {
      System.out.println("ğŸ”¹ Job " + jobStart.jobId() + " å¼€å§‹");
      System.out.println("   é˜¶æ®µæ•°é‡: " + jobStart.stageIds().size());

      // ä¿®å¤ï¼šæ­£ç¡®è½¬æ¢ Scala Seq
      List<Object> stageIds = JavaConverters.seqAsJavaListConverter(jobStart.stageIds()).asJava();
      System.out.println("   é˜¶æ®µIDs: " + stageIds);
    }

    @Override
    public void onStageSubmitted(SparkListenerStageSubmitted stageSubmitted) {
      StageInfo stageInfo = stageSubmitted.stageInfo();
      System.out.println("\nğŸ“¦ Stage " + stageInfo.stageId() + " æäº¤");
      System.out.println("   Stage åç§°: " + stageInfo.name());
      System.out.println("   ä»»åŠ¡æ•°é‡: " + stageInfo.numTasks());

      // ä¿®å¤ï¼šæ­£ç¡®è½¬æ¢çˆ¶é˜¶æ®µ IDs
      List<Object> parentIds = JavaConverters.seqAsJavaListConverter(stageInfo.parentIds()).asJava();
      System.out.println("   çˆ¶é˜¶æ®µ: " + parentIds);

      // åˆ†æä¾èµ–ç±»å‹
      if (parentIds.isEmpty()) {
        System.out.println("   ğŸ”— ä¾èµ–ç±»å‹: æ— ä¾èµ– (åˆå§‹é˜¶æ®µ)");
      } else if (parentIds.size() == 1) {
        System.out.println("   ğŸ”— ä¾èµ–ç±»å‹: çª„ä¾èµ–");
      } else {
        System.out.println("   ğŸ”— ä¾èµ–ç±»å‹: å®½ä¾èµ– (Shuffle)");
      }
    }

    @Override
    public void onStageCompleted(SparkListenerStageCompleted stageCompleted) {
      System.out.println("âœ… Stage " + stageCompleted.stageInfo().stageId() + " å®Œæˆ");
      latch.countDown();
    }
  }

  public static void main(String[] args) throws InterruptedException {
    SparkConf conf = new SparkConf()
      .setAppName("Visual Stage Division")
      .setMaster("local[2]")
      .set("spark.sql.adaptive.enabled", "false"); // å…³é—­è‡ªé€‚åº”æŸ¥è¯¢ï¼Œä¾¿äºè§‚å¯Ÿ

    JavaSparkContext sc = new JavaSparkContext(conf);
    sc.sc().addSparkListener(new DetailedListener());

    System.out.println("ğŸ¯ å¼€å§‹æ¼”ç¤º RDD Stage åˆ’åˆ†...\n");

    // æ¨¡æ‹Ÿå¤æ‚çš„æ•°æ®å¤„ç†æµç¨‹
    List<Integer> dataList = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16);
    JavaRDD<Integer> data = sc.parallelize(dataList, 4);

    System.out.println("åˆå§‹æ•°æ®åˆ†åŒºæ•°: " + data.getNumPartitions());

    // æ“ä½œé“¾1: çª„ä¾èµ–
    JavaRDD<Integer> processed1 = data
      .map(x -> x * 2)              // çª„ä¾èµ–
      .filter(x -> x > 10)          // çª„ä¾èµ–
      .map(x -> x + 1);             // çª„ä¾èµ–

    // æ“ä½œé“¾2: åŒ…å«å®½ä¾èµ–
    JavaPairRDD<Integer, Integer> pairRDD = processed1
      .mapToPair(x -> new Tuple2<>(x % 3, x));  // çª„ä¾èµ–

    JavaPairRDD<Integer, Integer> reducedRDD = pairRDD
      .reduceByKey(Integer::sum);            // å®½ä¾èµ– - Stage è¾¹ç•Œ

    JavaRDD<String> processed2 = reducedRDD
      .map(tuple -> "Key_" + tuple._1() + ": " + tuple._2()); // çª„ä¾èµ–

    // æ“ä½œé“¾3: å¦ä¸€ä¸ªå®½ä¾èµ–
    JavaRDD<String> finalResult = processed2
      .map(String::toUpperCase)                // çª„ä¾èµ–
      .repartition(2)                         // å®½ä¾èµ– - Stage è¾¹ç•Œ
      .filter(s -> s.contains("KEY_1"));      // çª„ä¾èµ–

    System.out.println("\nğŸš€ è§¦å‘è¡ŒåŠ¨æ“ä½œ...");
    List<String> results = finalResult.collect();

    System.out.println("\nğŸ“Š æœ€ç»ˆç»“æœ: " + results);

    // ç­‰å¾…æ‰€æœ‰é˜¶æ®µå®Œæˆ
    latch.await();

    System.out.println("\nğŸ‰ æ‰§è¡Œå®Œæˆï¼");
    System.out.println("\næ€»ç»“:");
    System.out.println("1. data -> map -> filter -> map: çª„ä¾èµ–ï¼Œä¸€ä¸ª Stage");
    System.out.println("2. mapToPair -> reduceByKey: reduceByKey äº§ç”Ÿå®½ä¾èµ–ï¼Œæ–°çš„ Stage");
    System.out.println("3. map -> repartition: repartition äº§ç”Ÿå®½ä¾èµ–ï¼Œæ–°çš„ Stage");
    System.out.println("4. filter -> collect: çª„ä¾èµ–ï¼ŒåŒä¸€ä¸ª Stage");

    sc.stop();
  }
}
